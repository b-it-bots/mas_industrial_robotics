#!/usr/bin/env python3
import os
import random
import time
import cv2
import numpy as np
import roslib
import rospy
import torch
import math
import csv
from geometry_msgs.msg import PoseStamped, Pose, Point, Quaternion
from cv_bridge import CvBridge, CvBridgeError
from std_msgs.msg import ColorRGBA, String
import message_filters
from mas_perception_msgs.msg import ImageList, Object, ObjectList, TimeStampedPose
from sensor_msgs.msg import Image, RegionOfInterest, PointCloud2, PointField
from sensor_msgs import point_cloud2
from ultralytics import YOLO
import tf
from sklearn.decomposition import PCA
from geometry_msgs.msg import PoseStamped, Pose
from visualization_msgs.msg import Marker
from scipy.optimize import least_squares
from sklearn.mixture import GaussianMixture
import tf.transformations as tr
from scipy.spatial.transform import Rotation as R
import tf2_sensor_msgs
from tf2_sensor_msgs.tf2_sensor_msgs import do_transform_cloud
import tf2_ros
import tf2_py as tf2
import ros_numpy
from scipy.optimize import least_squares
import pdb

class RTTObjectTracker():
    def __init__(self, weights, net='detection', model_name='yolov8',
                 confidence_threshold=0.8,
                 iou_threshold=0.45,
                 img_size=640,
                 trace=True,
                 classify=False,
                 augment=False,
                 device='cpu',
                 debug_mode=True):
        
        self.cvbridge = CvBridge()
        self.debug = debug_mode
        self.pub_debug = rospy.Publisher("/mir_perception/multimodal_object_recognition/recognizer/rgb/output/rtt_debug_image", Image, queue_size=1)
        self.pub_pose = rospy.Publisher("/mir_perception/mir_rtt_detector/predicted_object_pose", PoseStamped, queue_size=1)
        self.pub_cropped_pc = rospy.Publisher("rtt/output/cropped_pc", PointCloud2, queue_size=1)

        self.net = net
        self.model_name = model_name
        self.weights = weights
        self.confidence_threshold = confidence_threshold
        self.iou_threshold = iou_threshold
        self.augment = augment
        self.table_rotating_clockwise = False

        # Dictionary to store object bounding box centers
        self.object_centers = {}
        # Object pose at pickup location
        self.object_pose = {}
        # Number of latest bounding box centers to keep
        self.max_centers = 50
        # Dictionary to store object colors
        self.object_colors = {}
        # Dictionary to store object rotational velocities
        self.object_rotational_speed = 0
        # Dictionary to store object ellipse parameters
        self.object_ellipse_params_CL = {}
        self.object_ellipse_params_BL = {}
        self.settled_ellipse = {}
        # Number of rotations
        self.num_rotations = {}
        self.future_timestamps = {}
        self.center_point_position_list_wrt_base_link = {}
        self.object_pick_position = {}

        # Initialize
        self.device = torch.device(device)
        self.half = self.device.type != 'cpu'  # half precision only supported on CUDA

        # Load model
        if self.model_name == 'yolov8':
            self.model = YOLO(weights)
        self.img_size = img_size

        self.tf_buffer = tf2_ros.Buffer(cache_time=rospy.Duration(12))
        self.tf_listener = tf.TransformListener()
        self.tf2_listener = tf2_ros.TransformListener(self.tf_buffer)

        rospy.Subscriber("/mir_perception/rtt/event_in", String, self.event_in_cb)
        self.event_out = rospy.Publisher("/mir_perception/rtt/event_out", String, queue_size=1)
        self.time_stamped_pose = rospy.Publisher("/mir_perception/rtt/time_stamped_pose", TimeStampedPose, queue_size=5)

        self.event = None
        self.object_name = None
        self.time_estimation_complete = False
        self.orientation_estimation_complete = False
        self.pose_estimation_complete = False
        self.rotational_speed_calculation_complete = False

        # self.event_in_cb("e_start")


    def callback(self, img_msg, point_cloud_msg):
        try:
            self.run(img_msg, point_cloud_msg)
        except Exception as e:
            rospy.loginfo("[RTT detector node]killing point cloud callback")

    def event_in_cb(self, msg):
        """
        Starts a planned motion based on the specified arm position.

        """
        self.event = msg.data
        if self.event.startswith("e_start"):
        # if True:
            self.object_name = self.event[8:]

            # Subscribe to image topic
            self.sub_img = message_filters.Subscriber("/tower_cam3d_front/rgb/image_raw", Image)#, self.image_recognition_cb)

            # subscribe to point cloud topic
            self.sub_point_cloud = message_filters.Subscriber("/tower_cam3d_front/depth_registered/points", PointCloud2)#, self.point_cloud_cb)

            # synchronize image and point cloud
            self.ts = message_filters.ApproximateTimeSynchronizer([self.sub_img, self.sub_point_cloud], queue_size=10, slop=0.1)
            self.ts.registerCallback(self.callback)

        if self.event.startswith("e_stop"):
            # Dictionary to store object bounding box centers
            self.object_centers = {}
            # Object pose at pickup location
            self.object_pose = {}
            # Dictionary to store object rotational velocities
            self.object_rotational_speed = 0
            self.future_timestamps = {}
            self.center_point_position_list_wrt_base_link = {}
            self.object_pick_position = {}
            self.time_estimation_complete = False
            self.orientation_estimation_complete = False
            self.pose_estimation_complete = False
            self.rotational_speed_calculation_complete = False
            self.object_name = None
            self.sub_img.sub.unregister()
            self.sub_point_cloud.sub.unregister()
            rospy.loginfo("[RTT detector node] Unregistered image and pointcloud subscribers")

    def yolo_detect(self, cv_img):
        if self.net == 'detection' and self.model_name == 'yolov8':
            predictions = self.model.predict(source=cv_img,
                                                conf=self.confidence_threshold,
                                                iou=self.iou_threshold,
                                                device=self.device,
                                                verbose=False,
                                            )
            # convert results to numpy array
            predictions_np = predictions[0].boxes.numpy()
            class_ids = predictions_np.cls
            class_names = predictions[0].names
            class_labels = [class_names[i] for i in class_ids]
            class_scores = predictions_np.conf
            class_bboxes = predictions_np.xyxy # x, y, w, h

            return class_bboxes, class_scores, class_labels, predictions
        
    def drawAxis(self, img, p_, q_, color, scale):
        p = list(p_)
        q = list(q_)
        
        ## [visualization1]
        angle = math.atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
        hypotenuse = math.sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
        
        # Here we lengthen the arrow by a factor of scale
        q[0] = p[0] - scale * hypotenuse * math.cos(angle)
        q[1] = p[1] - scale * hypotenuse * math.sin(angle)
        cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 1, cv2.LINE_AA)
        
        # create the arrow hooks
        p[0] = q[0] + 9 * math.cos(angle + math.pi / 4)
        p[1] = q[1] + 9 * math.sin(angle + math.pi / 4)
        cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 1, cv2.LINE_AA)
        
        p[0] = q[0] + 9 * math.cos(angle - math.pi / 4)
        p[1] = q[1] + 9 * math.sin(angle - math.pi / 4)
        cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), color, 1, cv2.LINE_AA)
        return img
        
    def get_orientation(self, pts, img):
        ## [pca]
        # Construct a buffer used by the pca analysis
        sz = len(pts)
        data_pts = np.empty((sz, 2), dtype=np.float64)
        for i in range(data_pts.shape[0]):
            data_pts[i,0] = pts[i,0,0]
            data_pts[i,1] = pts[i,0,1]
        
        # Perform PCA analysis
        mean = np.empty((0))
        mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)
        
        # Store the center of the object
        cntr = (int(mean[0,0]), int(mean[0,1]))
        ## [pca]
        
        ## [visualization]
        # Draw the principal components
        # cv2.circle(img, cntr, 3, (255, 0, 255), 2)
        p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
        p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])
        img = self.drawAxis(img, cntr, p1, (255, 255, 0), 1)
        # img = self.drawAxis(img, cntr, p2, (0, 0, 255), 5)
        
        angle = math.atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
        return angle
    
    def circle_residuals(self, params, x, y):
        """
        Residual function for circle fitting.
        params: [a, b, r] where (a, b) is the center coordinates and r is the radius.
        x, y: Arrays of x and y coordinates of data points.
        """
        a, b, r = params
        return (x - a)**2 + (y - b)**2 - r**2

    def fit_circle(self, x, y):
        """
        Fit a circle to the given data points (x, y).
        Returns the center coordinates (a, b) and the radius r.
        """
        # Initial guess for the circle parameters
        initial_guess = [0.0, 0.0, 1.0]

        # Use least squares optimization to fit the circle
        result = least_squares(self.circle_residuals, initial_guess, args=(x, y))

        # Extract the optimized parameters
        center_x, center_y, radius = result.x

        return center_x, center_y, radius

    def get_pointcloud_position(self, pointcloud, center):
        """
        pointcloud: pointcloud data of current frame with PointCloud2 data type
        center: x and y coordinates of center of the bounding box of detected object
        """

        # Get the pointcloud data
        pc = np.array(list(point_cloud2.read_points(pointcloud, skip_nans=False, field_names=("x", "y", "z"))), dtype=np.float32)
        pc = pc.reshape((480,640,3))

        # Get the center point of the bounding box
        center_x, center_y = center
        # center_point = pc[center_y, center_x]

        # Define the radius of the circle
        radius = 7

        # Calculate the coordinates of the circle points within the radius
        min_x = max(0, center_x - radius)
        max_x = min(pc.shape[1] - 1, center_x + radius)
        min_y = max(0, center_y - radius)
        max_y = min(pc.shape[0] - 1, center_y + radius)

        circle_points = []
        for y in range(min_y, max_y + 1):
            for x in range(min_x, max_x + 1):
                distance = math.sqrt((center_x - x) ** 2 + (center_y - y) ** 2)
                if distance <= radius:
                    point = pc[y, x]
                    if not np.isnan(point).any():
                        circle_points.append(point)

        # Determine the centroid of the non-NaN points
        if circle_points:
            circle_points_pc = np.array([point for point in circle_points])
            center_point = np.mean(circle_points_pc, axis=0)

        # Get the pose of the center point of the bounding box
        pose = PoseStamped()
        pose.header.frame_id = pointcloud.header.frame_id
        pose.header.stamp = pointcloud.header.stamp
        pose.pose.position.x = center_point[0]
        pose.pose.position.y = center_point[1]
        pose.pose.position.z = center_point[2]
        pose.pose.orientation.x = 0.0
        pose.pose.orientation.y = 0.0
        pose.pose.orientation.z = 0.0
        pose.pose.orientation.w = 1.0

        try:
            t = self.tf_listener.getLatestCommonTime("base_link", pose.header.frame_id)
            pose.header.stamp = t
            pose = self.tf_listener.transformPose("base_link", pose)
            # if pose doesnot has nan values then append pose to the list
            if not math.isnan(pose.pose.position.x):
                center_wrt_BL = pose.pose.position.x, pose.pose.position.y
                return center_wrt_BL
        except (
            tf.LookupException,
            tf.ConnectivityException,
            tf.ExtrapolationException,
        ) as e:
            rospy.logerr("Tf error: %s" % str(e))

        return None
    
    def process_point_cloud(self, point_cloud, bounding_box):
        if point_cloud is None or bounding_box is None:
            return
        
        # Extract point cloud data within the bounding box
        cropped_cloud = self.crop_point_cloud(point_cloud, bounding_box)

        # Calculate the centroid (mean point) of the point cloud
        centroid = np.mean(cropped_cloud, axis=0)

        # normalize pointcloud with centroid
        cropped_cloud = cropped_cloud - centroid

        # Apply Principal Component Analysis (PCA)
        pca = PCA(n_components=3)
        pca.fit(cropped_cloud)

        # Get the eigenvalues and eigenvectors
        # eigenvalues = pca.explained_variance_
        eigenvectors = pca.components_
        # print("eigenvalues: ", eigenvalues, "\neigenvectors: \n", eigenvectors)

        # Calculate the pose using PCA results
        pose = self.calculate_pose(point_cloud, centroid, eigenvectors)
        self.orientation_estimation_complete = True
        return pose

    def crop_point_cloud(self, point_cloud, bounding_box):
        # Get the pointcloud data
        pc = np.array(list(point_cloud2.read_points(point_cloud, skip_nans=False, field_names=("x", "y", "z"))), dtype=np.float32)
        pc = pc.reshape((480,640,3))

        # crop the point cloud data within the bounding box
        cropped_cloud = pc[int(bounding_box[1]):int(bounding_box[3]), int(bounding_box[0]):int(bounding_box[2])]
        cropped_cloud_shape = cropped_cloud.shape

        # Ensure the cropped_cloud is a 2D array
        cropped_cloud = np.reshape(cropped_cloud, (-1, 3))
        
        # convert cropped_cloud data to ros pointcloud2 data type
        crp_cloud = np.core.records.fromarrays([cropped_cloud[:,0], cropped_cloud[:,1], cropped_cloud[:,2]], names='x, y, z')
        crp_pc = ros_numpy.point_cloud2.array_to_pointcloud2(crp_cloud, point_cloud.header.stamp, point_cloud.header.frame_id)
        
        try:
            trans = self.tf_buffer.lookup_transform("base_link", point_cloud.header.frame_id, point_cloud.header.stamp,
                                                    rospy.Duration(10))
        except tf2.LookupException as ex:
            rospy.logwarn(str(point_cloud.header.stamp.to_sec()))
            rospy.logwarn(ex)
            return
        except tf2.ExtrapolationException as ex:
            rospy.logwarn(str(point_cloud.header.stamp.to_sec()))
            rospy.logwarn(ex)
            return
        cropped_cloud_wrt_base_link = do_transform_cloud(crp_pc, trans)

        cropped_cloud = np.array(list(point_cloud2.read_points(cropped_cloud_wrt_base_link, skip_nans=False, field_names=("x", "y", "z"))), dtype=np.float32)
        cropped_cloud = cropped_cloud.reshape(cropped_cloud_shape)
        
        # get the min value of z in point cloud
        min_z = np.nanmin(cropped_cloud[:,:,2])
        
        # crop the z of cropped_cloud to remove the ground
        cropped_cloud = cropped_cloud[cropped_cloud[:,:,2] > min_z + 0.003]        

        # remove nan values
        cropped_cloud = cropped_cloud[~np.isnan(cropped_cloud)]

        # Check if the cropped_cloud array is empty
        if cropped_cloud.size == 0:
            return None

        # Ensure the cropped_cloud is a 2D array
        cropped_cloud = np.reshape(cropped_cloud, (-1, 3))
        
        if self.debug:
            # Publish the cropped point cloud data
            crp_cloud = np.core.records.fromarrays([cropped_cloud[:,0], cropped_cloud[:,1], cropped_cloud[:,2]], names='x, y, z')
            crp_pc = ros_numpy.point_cloud2.array_to_pointcloud2(crp_cloud, point_cloud.header.stamp, point_cloud.header.frame_id)
            self.pub_cropped_pc.publish(crp_pc)

        # Return the cropped point cloud data as a numpy array
        return np.array(cropped_cloud)

    def calculate_pose(self, pointcloud, centroid, eigenvectors):
        pose = PoseStamped()
        pose.header.stamp = pointcloud.header.stamp
        pose.header.frame_id = "base_link"

        # Set the position of the object
        pose.pose.position.x = centroid[0]
        pose.pose.position.y = centroid[1]
        pose.pose.position.z = centroid[2]

        # Swap largest and second largest eigenvectors
        # eigenvectors[:, [0, 2]] = eigenvectors[:, [2, 0]]

        # Compute the cross product of the second largest and largest eigenvectors
        # cross_product = np.cross(eigenvectors[:, 2], eigenvectors[:, 0])

        # Assign the cross product to the second largest eigenvector
        # eigenvectors[:, 1] = cross_product

        # pdb.set_trace()
        ## This is incorrect
        # rot_mat = np.identity(4)
        # rot_mat[0:3, 0:3] = eigenvectors.T
        # rot_mat[:3, 3] = -(rot_mat[:3, :3] @ centroid[:3])
        # quaternion = tr.quaternion_from_matrix(rot_mat)
        rot = R.from_matrix(eigenvectors.T)
        quaternion = rot.as_quat()
        # eular = rot.as_euler('ZYX', degrees=True)
        # print(eular)
        
        roll, pitch, yaw = tr.euler_from_quaternion(quaternion)
        orientation = tr.quaternion_from_euler(0, 0, yaw)
        # print("pose = ", pose.pose.orientation)
        pose.pose.orientation.x = orientation[0]
        pose.pose.orientation.y = orientation[1]
        pose.pose.orientation.z = orientation[2]
        pose.pose.orientation.w = orientation[3]

        self.pub_pose.publish(pose)

        return pose

    def get_center_and_yaw(self, point_cloud, bounding_box):
        if point_cloud is None or bounding_box is None:
            return
        
        # Extract point cloud data within the bounding box
        cropped_cloud = self.crop_point_cloud(point_cloud, bounding_box)

        # Calculate the centroid (mean point) of the point cloud
        centroid = np.mean(cropped_cloud, axis=0)

        # normalize pointcloud with centroid
        cropped_cloud = cropped_cloud - centroid

        # Apply Principal Component Analysis (PCA)
        pca = PCA(n_components=3)
        pca.fit(cropped_cloud)

        # Get the eigenvalues and eigenvectors
        eigenvectors = pca.components_
        # print("eigenvalues: ", eigenvalues, "\neigenvectors: \n", eigenvectors)

        rot = R.from_matrix(eigenvectors.T)
        quaternion = rot.as_quat()
        
        roll, pitch, yaw = tr.euler_from_quaternion(quaternion)

        return (centroid[0], centroid[1]), yaw
    
    def predict_yaw(self, xc, yc, center_list, yaw_list_CL):
        """
        xc, yc: center point of the circle
        center_list: list of center points of the detected object in each frame
        yaw_list: list of yaw angles of the detected object in each frame
        """
        
        # convert yaw_list_CL to yaw_list_BL
        yaw_list_BL = yaw_list_CL
        for i in range(len(yaw_list_CL)):
            yaw_list_BL[i] = np.pi/2 - yaw_list_CL[i] # from CL to BL
            if yaw_list_BL[i] > np.pi:
                yaw_list_BL[i] -= 2*np.pi    
            if yaw_list_BL[i] < 0:
                yaw_list_BL[i] += np.pi
            # now yaw_list is b/w 0 to np.pi (180 degrees)

        # predict at the point of pickup (at Base Link angle of 180 degrees)
        for i in range(len(center_list)):
            xo, yo = center_list[i]
            angle = math.atan2(yo - yc, xo - xc)
            yaw_list_BL[i] -= angle
            if yaw_list_BL[i] >= np.pi:
                yaw_list_BL[i] -= np.pi
            elif yaw_list_BL[i] < 0:
                yaw_list_BL[i] += np.pi

        # get median of yaw_list
        predicted_yaw_BL = np.median(np.array(yaw_list_BL))
        return predicted_yaw_BL
    
    def check_stability(self, params_list):
        # Check if the standard deviation of the parameters is below a threshold
        threshold = 0.25  # Adjust the threshold as per your requirements

        if len(params_list) < 2:
            return False

        params_array = np.array(params_list[-5:])
        std_dev = np.std(params_array, axis=0)
        stability = np.all(std_dev < threshold)
        # print("stability std_dev: ", std_dev)

        return stability

    def calculate_rotational_speed(self, center_list, frame_time_list, xc, yc, label):
        """
        center_list: list of center points of the detected object in each frame
        frame_time_list: list of time stamps of each frame
        xc, yc: center point of the circle
        """
        # self.num_rotations[label] = -1
        # Calculate the angle between each center point and the ellipse center
        angle_list = []
        for center in center_list:
            xo, yo = center
            angle = math.atan2(yo - yc, xo - xc)
            angle_list.append(angle)

        # Calculate the time differences between two consecutive frames
        time_diff_list = []
        for i in range(1, len(frame_time_list)):
            time_diff = frame_time_list[i] - frame_time_list[i - 1]
            time_diff_list.append(time_diff)

        # Calculate the rotational speeds based on the angle differences and time differences
        rotational_speed_list = []
        count_clockwise = 0
        count_anticlockwise = 0
        # print("is self.table_rotating_clockwise? : ", self.table_rotating_clockwise)
        # print("num_rotations: ", " for \"", label, "\" is: ", self.num_rotations[label])
        
        for i in range(len(angle_list) - 1):
            # detection of rotation direction
            if 0 <= angle_list[i] <= math.pi and 0 <= angle_list[i+1] <= math.pi and (count_anticlockwise + count_clockwise < 5):
                if angle_list[i] < angle_list[i+1]:
                    count_anticlockwise += 1
                else:
                    count_clockwise += 1
            if count_clockwise > count_anticlockwise:
                self.table_rotating_clockwise = True
            else:
                self.table_rotating_clockwise = False

            # # count the number of rotations
            # if (not self.table_rotating_clockwise and (math.pi/2 < abs(angle_list[i]) < math.pi and 0 < abs(angle_list[i+1]) < math.pi/2)) or \
            #         (self.table_rotating_clockwise and (0.0 < abs(angle_list[i]) < math.pi/2 and math.pi/2 < abs(angle_list[i+1]) < math.pi)):
            #     self.num_rotations[label] += 1

            # if object is in the quadrant adjacent to pickup point, calculate the angle difference and rotational speed, and get the average value
            if abs(angle_list[i]) >= math.pi/2 and abs(angle_list[i+1]) >= math.pi/2:
                # check if product of two consecutive angles is negative
                if angle_list[i] * angle_list[i+1] < 0:
                    if angle_list[i] > 0:
                        angle_diff = abs(angle_list[i] - (angle_list[i+1] + 2*math.pi))
                    else:
                        angle_diff = abs(angle_list[i+1] - (angle_list[i] + 2*math.pi))
                else:
                    angle_diff = abs(angle_list[i + 1] - angle_list[i])
                time_diff = frame_time_list[i+1] - frame_time_list[i]
                if time_diff < 3 and time_diff > 0.05:
                    rotational_speed = abs(angle_diff) / time_diff
                    rotational_speed_list.append(rotational_speed)

        # return the average value for rotational speed
        if len(rotational_speed_list) == 0:
            return 0
        else:
            rotational_speed_array = np.array(rotational_speed_list)
            avg_rotational_speed = np.mean(rotational_speed_array)
            # print("avg_rotational_speed: ", avg_rotational_speed)

            return avg_rotational_speed

    def predict_future_timestamps(self, avg_rotational_speed, center_list, frame_time_list, xc, yc, closest_x, closest_y):
        """
        center_list: list of center points of the detected object in each frame
        frame_time_list: list of time stamps of each frame
        xc, yc: center point of the ellipse
        avg_rotational_speed: rotational spee+6+d of particular object
        closest_x, closest_y: closest point on the ellipse to the end-effector
        """

        last_n_locations = center_list[-5:]  # Select the last 5 center points
        last_n_frame_times = frame_time_list[-5:]  # Select the corresponding frame time stamps
        closest_angle = math.pi
        time_for_one_rotation = 2 * math.pi / avg_rotational_speed
        future_time_stamps = []

        for i in range(len(last_n_locations)):
            latest_location_x = last_n_locations[i][0]
            latest_location_y = last_n_locations[i][1]
            latest_frame_time = last_n_frame_times[i]

            last_angle = math.atan2(latest_location_y - yc, latest_location_x - xc)

            # Calculate the difference between angles, considering the range of last angle
            if self.table_rotating_clockwise:
                angle_difference = abs(2*math.pi - abs(closest_angle - last_angle))
            else:
                angle_difference = abs(closest_angle - last_angle)

            time_diff = angle_difference / avg_rotational_speed
            next_time_stamp = latest_frame_time + time_diff
            future_time_stamps.append(next_time_stamp)

        # Calculate median of the future time stamps
        median_future_time_stamp = np.median(np.array(future_time_stamps))
        if median_future_time_stamp > rospy.Time.now().to_sec() + 3.0: 
            return median_future_time_stamp
        else:
            median_future_time_stamp += time_for_one_rotation
            return median_future_time_stamp
        
    def run(self, image, pointcloud):
        """
        image: image data of current frame with Image data type
        pointcloud: pointcloud data of current frame with PointCloud2 data type
        """

        start = time.time()
        if image:
            try:
                cv_img = self.cvbridge.imgmsg_to_cv2(image, "bgr8")
                bboxes, probs, labels, predictions = self.yolo_detect(cv_img)
                # Capatilize labels
                labels = [label.upper() for label in labels]

                image_header_frame_time = image.header.stamp
                frame_time = image_header_frame_time.secs + \
                    (image_header_frame_time.nsecs / 1000000000)
                
                bbox_img = None

                # Fit a Ellipse to the centers of the bounding boxes
                for bbox, prob, label in zip(bboxes, probs, labels):
                    if label == self.object_name:
                        # Calculate center of bounding box
                        center_x = (int(bbox[0]) + int(bbox[2])) // 2
                        center_y = (int(bbox[1]) + int(bbox[3])) // 2
                        center = np.array([center_x, center_y])

                        # Convert image to grayscale
                        bbox_img = cv_img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]
                        if bbox_img is None:
                            continue
                        gray = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)
                        
                        # Convert image to binary
                        _, bw = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
                        
                        # Find all the contours in the thresholded image
                        contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

                        # get contour with maximum area
                        max_area = 0
                        best_cnt = None
                        for cnt in contours:
                            area = cv2.contourArea(cnt)
                            if area > max_area:
                                max_area = area
                                best_cnt = cnt
                        
                        yaw_angle = self.get_orientation(best_cnt, bbox_img)

                        # Update object centers dictionary
                        if label not in self.center_point_position_list_wrt_base_link and center_y > 30:
                            self.object_centers[label] = []
                            self.object_ellipse_params_CL[label] = []
                            self.object_ellipse_params_BL[label] = []
                            self.num_rotations[label] = -1
                            self.future_timestamps[label] = None
                            self.settled_ellipse[label] = False
                            self.object_pose[label] = None
                            self.center_point_position_list_wrt_base_link[label] = []
                            self.object_pick_position[label] = None
                        if label in self.center_point_position_list_wrt_base_link and center_y > 20:
                            self.object_centers[label].append((center, frame_time))
                            center_BL = self.get_pointcloud_position(pointcloud, center)
                            # center_BL, yaw_angle = self.get_center_and_yaw(pointcloud, bbox)
                            if center_BL[0] is not None:
                                self.center_point_position_list_wrt_base_link[label].append((center_BL, yaw_angle, frame_time))
                            # Keep only the last `max_centers` centers
                            if len(self.object_centers[label]) > self.max_centers:
                                self.object_centers[label] = self.object_centers[label][-self.max_centers:]
                                self.center_point_position_list_wrt_base_link[label] = self.center_point_position_list_wrt_base_link[label][-self.max_centers:]
                        
                        # # Create a CSV file for base link frame points
                        # with open('/home/robocup/ros/noetic/robocup/object_data_wrt_base_link.csv', mode='w') as file:
                        #     writer = csv.writer(file)
                        #     writer.writerow(['Object Name', 'X', 'Y', 'yaw', 'Frame Time'])

                        #     # Write the object data to the CSV file
                        #     for label, centers_data in self.center_point_position_list_wrt_base_link.items():
                        #         for center, yaw, frame_time in centers_data:
                        #             # print(f"Label: {label}, Center: {center}, Frame Time: {frame_time}")
                        #             writer.writerow([label, center[0], center[1], yaw, frame_time])
                        
                        if label in self.center_point_position_list_wrt_base_link:
                            if self.center_point_position_list_wrt_base_link[label] and len(self.center_point_position_list_wrt_base_link[label]) > 15:
                                # print("\n RTT node ready to pick object")
                                for label, centers_data in self.center_point_position_list_wrt_base_link.items():
                                    centers = np.array([center for center, _, _ in centers_data])
                                    yaw_list = [yaw for _, yaw, _ in centers_data]
                                    frame_time_list = [frame_time for _, _, frame_time in centers_data]
                                    xc, yc, radius = self.fit_circle(centers[:,0], centers[:,1])
                                    
                                    # getting the point on the trajectory at 90 degrees in image plane [x-axis: left to right, y-axis: top to bottom]
                                    closest_x = xc - radius + 0.014
                                    closest_y = yc
                                    
                                    if np.isnan(closest_x):
                                        continue
                                    if not math.isnan(closest_x):
                                        # the pose of the object is calculated by cropping pointcloud by bounding box and applying PCA on it
                                        self.object_pose[label] = PoseStamped()
                                        self.object_pose[label].header.stamp = pointcloud.header.stamp
                                        self.object_pose[label].header.frame_id = "base_link"
                                        self.object_pose[label].pose.position.x = closest_x
                                        self.object_pose[label].pose.position.y = closest_y
                                        self.object_pose[label].pose.position.z = 0.005
                                                
                                        if label == "M30" or label == "M20" or label == "BEARING2":
                                            self.object_pose[label].pose.orientation.x = 0.0
                                            self.object_pose[label].pose.orientation.y = 0.0
                                            self.object_pose[label].pose.orientation.z = 0.0
                                            self.object_pose[label].pose.orientation.w = 1.0
                                        else:
                                            predicted_yaw = self.predict_yaw(xc, yc, centers, yaw_list)
                                            orientation = tr.quaternion_from_euler(0, 0, predicted_yaw)
                                            self.object_pose[label].pose.orientation.x = orientation[0]
                                            self.object_pose[label].pose.orientation.y = orientation[1]
                                            self.object_pose[label].pose.orientation.z = orientation[2]
                                            self.object_pose[label].pose.orientation.w = orientation[3]
                                        self.orientation_estimation_complete = True

                                    if not self.rotational_speed_calculation_complete:
                                        # get rotational speed
                                        self.object_rotational_speed = self.calculate_rotational_speed(centers, frame_time_list, xc, yc, label)
                                        self.rotational_speed_calculation_complete = True
                                    # print("self.object_rotational_speed: ", self.object_rotational_speed)
                                    if self.rotational_speed_calculation_complete and self.object_rotational_speed != 0:
                                        # predict future timestamps at (closest_x, closest_y)
                                        self.future_timestamps[label] = self.predict_future_timestamps(self.object_rotational_speed, centers, frame_time_list, xc, yc, closest_x, closest_y)
                                        self.time_estimation_complete = True
                                        if self.time_estimation_complete and self.orientation_estimation_complete and self.future_timestamps[self.object_name] is not None:
                                            self.time_stamped_pose_msg = TimeStampedPose()
                                            self.time_stamped_pose_msg.timestamps = self.future_timestamps[self.object_name]
                                            self.time_stamped_pose_msg.pose = self.object_pose[self.object_name]
                                            # print("RTT - Object pickup timestamp for label ", label, " is: ", self.future_timestamps[label],  
                                            #     "\nCurrent time for label ", label, " is: ", rospy.Time.now().to_sec())
                                            # print(self.time_stamped_pose_msg)
                                            self.pub_pose.publish(self.object_pose[self.object_name])
                                            self.time_stamped_pose.publish(self.time_stamped_pose_msg)
                                    else:
                                        self.time_estimation_complete = False
                    else:
                        continue
                if self.debug:
                    # Draw bounding boxes and labels of detections
                    debug_img = predictions[0].plot()

                    # publish bbox and label
                    # self.publish_debug_img(debug_img)
                    if bbox_img is not None:
                        self.publish_debug_img(bbox_img)

                end = time.time()
                # rospy.loginfo("Time taken for detection: %s",str(end - start))

            except CvBridgeError as e:
                rospy.logerr(e)
                return
        else:
            rospy.logwarn("No image received")

    def publish_debug_img(self, debug_img):
        debug_img = np.array(debug_img, dtype=np.uint8)
        debug_img = self.cvbridge.cv2_to_imgmsg(debug_img, "bgr8")
        self.pub_debug.publish(debug_img)


if __name__ == '__main__':
    rospy.init_node("rtt")
    rospy.loginfo('Started RTT Node.')
    net = rospy.get_param("/mir_perception/multimodal_object_recognition/recognizer/rgb/rtt_detector/net")
    classifier_name = rospy.get_param("/mir_perception/multimodal_object_recognition/recognizer/rgb/rtt_detector/classifier")
    dataset = rospy.get_param("/mir_perception/multimodal_object_recognition/recognizer/rgb/rtt_detector/dataset")
    model_dir = rospy.get_param("/mir_perception/multimodal_object_recognition/recognizer/rgb/rtt_detector/model_dir")
    weights = os.path.join(roslib.packages.get_pkg_dir("mir_rgb_object_recognition_models"),
                           'common', 'models', classifier_name, dataset, "atwork.pt")
    object_recognizer = RTTObjectTracker(
        weights=weights, debug_mode=True)
    rospy.loginfo("RTT Recognizer is ready using %s : %s , dataset: %s ",
                  net, classifier_name, dataset)
    rospy.spin()
